{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02364054 0.06426166 0.1746813  0.474833   0.02364054 0.06426166\n",
      " 0.1746813 ]\n"
     ]
    }
   ],
   "source": [
    "#implement softmax\n",
    "import numpy as np\n",
    "\n",
    "def softmax_1d(x):  \n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "x=[1, 2, 3, 4, 1, 2, 3]\n",
    "print(softmax_1d(x)) #[0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894142 0.73105858]\n"
     ]
    }
   ],
   "source": [
    "test1 = softmax_1d(np.array([1,2]))\n",
    "print (test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02364054 0.06426166 0.1746813  0.474833   0.02364054 0.06426166\n",
      " 0.1746813 ]\n"
     ]
    }
   ],
   "source": [
    "#numerical stable version, subtract max(x)\n",
    "def softmax_1d(x):  \n",
    "    return np.exp(x-np.max(x))/np.sum(np.exp(x-np.max(x)))\n",
    "x=[1, 2, 3, 4, 1, 2, 3]\n",
    "print(softmax_1d(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.max: 1002\n",
      "[[1001 1002]]\n",
      "[[1002]\n",
      " [   4]]\n",
      "[[0.36787944 1.        ]\n",
      " [0.36787944 1.        ]]\n",
      "[[0.13447071 0.36552929]\n",
      " [0.13447071 0.36552929]]\n"
     ]
    }
   ],
   "source": [
    "#the numerically stable version is defined for subtracting np.max from each row!\n",
    "#\n",
    "x = np.array([[1001,1002],[3,4]])\n",
    "\n",
    "print(\"np.max:\",np.max(x))\n",
    "\n",
    "#there are 3 np.max; which one do you use? Depends on derivation; how to preserve operation. \n",
    "print(np.max(x,axis=0, keepdims=True))\n",
    "print(np.max(x,axis=1, keepdims=True))\n",
    "print(np.exp(x-np.max(x,axis=1, keepdims=True)))\n",
    "print(np.exp(x-np.max(x,axis=1, keepdims=True))/np.sum(np.exp(x-np.max(x,axis=1, keepdims=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2bd503db51fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# dito\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0me_x\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1001,1002],[3,4]])\n",
    "def softmax_correct(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_broadcast(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26894142 0.73105858]\n",
      " [0.26894142 0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "#this is correct bc you have to find the np.max for each row!\n",
    "f=x\n",
    "f -= np.max(f, axis=1, keepdims=True) # max of every sample\n",
    "sum_f = np.sum(np.exp(f), axis=1, keepdims=True)\n",
    "p = np.exp(f) / sum_f\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[-1  0]\n",
      " [-1  0]]\n",
      "x-np.max: [[0.36787944 1.        ]\n",
      " [0.36787944 1.        ]]\n",
      "sum_e [-2  0]\n",
      "e.sum azis0 [-2  0]\n",
      "[[0.5 nan]\n",
      " [0.5 nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#print(x-np.max(x))\n",
    "print('x:',x)\n",
    "print('x-np.max:',np.exp(x-np.max(x)))\n",
    "e = x-np.max(x)\n",
    "sum_e = np.sum(e,axis=0)\n",
    "print('sum_e',sum_e)\n",
    "print('e.sum azis0',e.sum(axis=0))\n",
    "print(e/sum_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  0]\n",
      " [-1  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_2d(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "print(x)\n",
    "softmax_2d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1ee8255b4a00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      [1, 2, 3, 6]])\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# [[ 0.09003057  0.00242826  0.01587624  0.33333333]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#  [ 0.24472847  0.01794253  0.11731043  0.33333333]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "#higher dimensions test by seeing row #1 same as row#3. Use patterns. Hard to see correct/incorrct\n",
    "#with different values in all rows/columns\n",
    "scores2D = np.array([[1, 2, 3, 6],\n",
    "                     [2, 4, 5, 6],\n",
    "                     [1, 2, 3, 6]])\n",
    "print(softmax(scores2D))\n",
    "# [[ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
    "#  [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
    "#  [ 0.66524096  0.97962921  0.86681333  0.33333333]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cd33744f4ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1002\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "softmax(np.array([[1001,1002],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (0, 0)\n",
      "3 (0, 1)\n",
      "4 (0, 2)\n",
      "4 (1, 0)\n",
      "5 (1, 1)\n",
      "6 (1, 2)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[3,4,5]])\n",
    "it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "while not it.finished:\n",
    "    ix = it.multi_index #how to use ix? \n",
    "    it[0][...] +=1\n",
    "    print(it[0],it.multi_index)\n",
    "    it.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" Gradient check for a function f using difference \n",
    "\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        # Try modifying x[ix] with h defined above to compute\n",
    "        # numerical gradients. Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later.\n",
    "\n",
    "        ### YOUR CODE HERE:\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad)\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3d535a964cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-3d535a964cd7>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + x + 1\n",
    "def grad_f(x):\n",
    "    return 2*x + 1\n",
    "print(f(4))\n",
    "print(grad_f(4))\n",
    "#print(check_grad(f,grad_f,[4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "\n",
    "\n",
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Arguments:\n",
    "    data -- M x Dx matrix, where each row is a training example.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = np.exp(x)/(np.exp(x) + 1)\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input s should be the sigmoid\n",
    "    function value of your original input x.\n",
    "\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ds = sigmoid(s) * (1-sigmoid(s))\n",
    "    #raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [-1 -2]]\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.19661193 0.10499359]\n",
      " [0.19661193 0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2], [-1, -2]])\n",
    "f = sigmoid(x)\n",
    "g = sigmoid_grad(x)\n",
    "print(x)\n",
    "print(f)\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.21936186 0.20715623]\n",
      " [0.24553335 0.24911402]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-02c0f2911cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-06\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"You should verify these results by hand!\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_sigmoid_basic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-02c0f2911cd7>\u001b[0m in \u001b[0;36mtest_sigmoid_basic\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m0.19661193\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.10499359\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         [0.19661193, 0.10499359]])\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-06\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"You should verify these results by hand!\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtest_sigmoid_basic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print (f)\n",
    "    f_ans = np.array([\n",
    "        [0.73105858, 0.88079708],\n",
    "        [0.26894142, 0.11920292]])\n",
    "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
    "    print (g)\n",
    "    g_ans = np.array([\n",
    "        [0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])\n",
    "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n",
    "    print (\"You should verify these results by hand!\\n\")\n",
    "test_sigmoid_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d7e12265f0b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreebank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordSentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs224n/assignment1/utils/treebank.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from q3_word2vec import *\n",
    "from q3_sgd import *\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingCostAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print (\"sanity check: cost at convergence should be around or below 10\")\n",
    "print (\"training took %d seconds\" % (time.time() - startTime))\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "# wordVectors = wordVectors[:nWords,:] + wordVectors[nWords:,:]\n",
    "visualizeWords = [\n",
    "    \"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\",\n",
    "    \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\",\n",
    "    \"annoying\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a cost and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "     postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "     if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    expcost = None\n",
    "     for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        # Don't forget to apply the postprocessing after every iteration!\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print \"iter %d: %f\" % (iter, expcost)\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "        \n",
    "    return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 1 result:\", t1\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 2 result:\", t2\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 3 result:\", t3\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement word2vec\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]]))\n",
    "    print x\n",
    "    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"\"\n",
    "\n",
    "    \n",
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in xrange(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currrentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "\n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "\n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "\n",
    "    Extra credit: Implementing CBOW is optional, but the gradient\n",
    "    derivations are not. If you decide not to implement CBOW, remove\n",
    "    the NotImplementedError.\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "           \n",
    "           \n",
    "           \n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n",
    "                         word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "\n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerword, C1, context, tokens, inputVectors, outputVectors,\n",
    "            dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "\n",
    "    return cost, grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)\n",
    "     print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMITSALHRAADWAKSVFSSAALGDPRSTARLVNVAAQLAKYSGKSITISSEGSKAMQEGAYRFIRNPNVSAEAIRKAGAMQTVKLAQEFPELLATEDTTSLSYRHQVAEELGKLGSIQDKSRGWWVHSVLLLEATTFRTVGLLHQEWWMRPDDPADADEKESGKWLAAAATSRLRMGSMMSNVIAVCDREADIHAYLQDKLAHNERFVVRSKHPRKDVESGLYLYDHLKNQPELGGYQTSIPQKGVVDKRGKRKNRPARKASLSLRSGRITLKQGNITLNAVLAEEINPPKGETPLKWLLLTSEPVESLAQALRVIDIYTHRWRIEEFHKAWKTGAGAERQRMEEPDNLERMVSILSFVAVRLLQLRESFTPPQALRAQGLLKEAEHVESQSAETVLTPDECQLLGYLDKGKRKRKEKAGSLQWAYMAIARLGGFMDSKRTGIASWGALWEGWEALQSKLDGFLAAKDLMAQGIKI\n"
     ]
    }
   ],
   "source": [
    "test_string=\"SMITSALHRA ADWAKSVFSS AALGDPRSTA RLVNVAAQLA KYSGKSITIS \\\n",
    "SEGSKAMQEG AYRFIRNPNV SAEAIRKAGA MQTVKLAQEF PELLATEDTT \\\n",
    "SLSYRHQVAE ELGKLGSIQD KSRGWWVHSV LLLEATTFRT VGLLHQEWWM \\\n",
    "RPDDPADADE KESGKWLAAA ATSRLRMGSM MSNVIAVCDR EADIHAYLQD \\\n",
    "KLAHNERFVV RSKHPRKDVE SGLYLYDHLK NQPELGGYQT SIPQKGVVDK \\\n",
    "RGKRKNRPAR KASLSLRSGR ITLKQGNITL NAVLAEEINP PKGETPLKWL \\\n",
    "LLTSEPVESL AQALRVIDIY THRWRIEEFH KAWKTGAGAE RQRMEEPDNL \\\n",
    "ERMVSILSFV AVRLLQLRES FTPPQALRAQ GLLKEAEHVE SQSAETVLTP \\\n",
    "DECQLLGYLD KGKRKRKEKA GSLQWAYMAI ARLGGFMDSK RTGIASWGAL \\\n",
    "WEGWEALQSK LDGFLAAKDL MAQGIKI\"\n",
    "\n",
    "len(test_string.replace(\" \",''))\n",
    "print(test_string.replace(\" \",''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
